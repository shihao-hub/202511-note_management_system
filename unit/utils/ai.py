"""
ai ç›¸å…³å·¥å…·

"""
import asyncio
import os
import traceback
import functools
from abc import ABC, abstractmethod
from pathlib import Path
from typing import TypedDict, Dict, Union, List

from loguru import logger
from aiohttp import ClientSession, ClientTimeout
from result import Result, Ok, Err
from openai import AsyncOpenAI
from gradio_client import Client, handle_file, FileData

from .mediator import get_thread_pool_executor


class AIHandlerResponseTypedDict(TypedDict):
    source: str
    response: str
    raw: Dict | None  # å¾…å®šï¼Œæš‚ä¸ä½¿ç”¨


class AsyncAIHandler(ABC):
    """è®¾è®¡æ¨¡å¼ - è´£ä»»é“¾æ¨¡å¼ï¼ˆä½†æ˜¯ä¸å¤Ÿå®Œæ•´ï¼Œç›®å‰å¤Ÿç”¨äº†ï¼‰"""

    def __init__(self):
        self._next_handler: Union["AsyncAIHandler", None] = None

    def set_next(self, handler: "AsyncAIHandler") -> "AsyncAIHandler":
        """è®¾ç½®åœ¨æ­¤ä¹‹åçš„ä¸‹ä¸€ä¸ªå¤„ç†å™¨"""
        self._next_handler = handler
        return handler

    @abstractmethod
    async def handle(self, prompt: str) -> AIHandlerResponseTypedDict | None:
        """å¾…å®ç°çš„å¤„ç†å™¨æ‰§è¡Œé€»è¾‘"""
        # [2025-11-19] ç›®å‰é»˜è®¤éƒ½æ˜¯éæµå¼çš„
        # todo: æ³¨æ„ï¼Œè¿™ä¸ªè´£ä»»é“¾ä¸å¤Ÿå®Œå–„ï¼Œå› ä¸º handle é»˜è®¤è¿˜éœ€è¦è‡ªå·±æ‰§è¡Œä¸€äº›ä»£ç ï¼Œä¸å¥½


class LocalOllamaHandler(AsyncAIHandler):
    url = "http://localhost:11434/api/generate"
    timeout = 8  # ç­‰å¾…å“åº”è¿”å›æ—¶é—´ï¼ˆå› ä¸ºéæµå¼ï¼Œè€Œä¸”æœ¬åœ° ollama å¯èƒ½è¿˜å­˜åœ¨å“åº”æ—¶é—´ï¼‰
    model = "qwen2.5:3b"
    stream = False

    async def handle(self, prompt: str) -> AIHandlerResponseTypedDict | None:
        try:
            logger.debug("[LocalOllamaHandler:handle] ğŸ”„ å°è¯•è°ƒç”¨æœ¬åœ° Ollama (localhost:11434)...")
            payload = {"model": LocalOllamaHandler.model, "prompt": prompt, "stream": LocalOllamaHandler.stream}
            async with ClientSession() as session:
                async with session.post(
                        LocalOllamaHandler.url,
                        json=payload,
                        timeout=ClientTimeout(total=LocalOllamaHandler.timeout)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        # Ollama è¿”å›çš„æ˜¯å•ä¸ª JSON å¯¹è±¡ï¼ˆéæµå¼ï¼‰
                        logger.debug("[LocalOllamaHandler:handle] âœ… æœ¬åœ° Ollama æˆåŠŸå“åº”")
                        result: AIHandlerResponseTypedDict = {
                            "source": f"local-ollama/{LocalOllamaHandler.model}",
                            "response": data.get("response", ""),
                            "raw": data
                        }
                        return result
                    else:
                        text = await response.text()
                        logger.debug(
                            f"[LocalOllamaHandler:handle] âš ï¸ æœ¬åœ° Ollama è¿”å›é”™è¯¯çŠ¶æ€ç  {response.status}: {text}")
        except Exception as e:
            logger.error(f"[LocalOllamaHandler:handle] âŒ æœ¬åœ° Ollama è¯·æ±‚å¤±è´¥: {e}")

        if self._next_handler:
            return await self._next_handler.handle(prompt)

        return None


class DeepSeekHandler(AsyncAIHandler):
    async def handle(self, prompt: str) -> AIHandlerResponseTypedDict | None:
        try:
            logger.debug("[DeepSeekHandler:handle] ğŸ”„ å°è¯•è°ƒç”¨ DeepSeek ...")
            async with DeepSeekClient() as client:
                text_result = await client.ask_ai(prompt)
            if text_result.is_err():
                raise Exception(text_result.err())
            text = text_result.unwrap()
            logger.debug("[DeepSeekHandler:handle] âœ… DeepSeek æˆåŠŸå“åº”")
            result: AIHandlerResponseTypedDict = {
                "source": f"deepseek/{client.model}",
                "response": text,
                "raw": None
            }
            return result
        except Exception as e:
            logger.error(f"[DeepSeekHandler:handle] âŒ DeepSeek è¯·æ±‚å¤±è´¥: {e}")
            logger.error(traceback.format_exc())

        if self._next_handler:
            return await self._next_handler.handle(prompt)

        return None


@functools.lru_cache(maxsize=None)
def build_ai_chain() -> AsyncAIHandler:
    """æ„å»º ai è°ƒç”¨é“¾"""
    # https://lxblog.com/qianwen/share?shareId=efdbbb73-3dbb-48a2-b3a4-46d26472965b
    handlers: List[AsyncAIHandler] = [
        LocalOllamaHandler(),
        DeepSeekHandler(),
    ]
    # é“¾æ¥å¤„ç†å™¨
    for i in range(len(handlers) - 1):
        handlers[i].set_next(handlers[i + 1])
    return handlers[0]


class DeepSeekClient:
    def __init__(self, model: str = "deepseek-chat"):
        self.model = model

        # ä½¿ç”¨æ‡’åŠ è½½ï¼Œé¿å…é¡¹ç›®æ— æ³•å¯åŠ¨
        self._api_key: str | None = None
        self._client: AsyncOpenAI | None = None

    @property
    def api_key(self):
        if self._api_key is None:
            self._api_key = os.environ.get("DEEPSEEK_API_KEY")
            if not self._api_key:
                raise ValueError("API key must be provided via argument or DEEPSEEK_API_KEY environment variable.")
        return self._api_key

    @property
    def client(self):
        if self._client is None:
            self._client = AsyncOpenAI(api_key=self.api_key, base_url="https://api.deepseek.com")
        return self._client

    async def __aenter__(self) -> "DeepSeekClient":
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.client:
            await self.client.close()
        return False

    async def ask_ai(self, user_content: str, *, system_content: str = "") -> Result[str, str]:
        try:
            params = dict(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": f"{user_content}"},
                ],
                stream=False
            )
            response = await self.client.chat.completions.create(**params)  # messages çš„æ ¼å¼ä¼šæç¤ºé”™è¯¯ï¼Œæ‰€ä»¥é€‰æ‹©è¿™æ ·å¤„ç†
            text = response.choices[0].message.content.strip()
            # logger.debug("[_ai_generate] text: {}", text)
            return Ok(text)
        except Exception as e:
            logger.error(e)
            return Err(str(e))

    async def ai_generate_title(self, user_content: str) -> Result[str, str]:
        # ä¸´æ—¶çš„
        system_content = """
        ä½ æ˜¯ä¸€ä½æ–‡æœ¬æ€»ç»“ä¸“å®¶ï¼Œä½ éœ€è¦å°†ç”¨æˆ·å‘é€çš„å†…å®¹æ€»ç»“æˆä¸€ä¸ªç®€çŸ­çš„æ ‡é¢˜ï¼ˆä¸è¦è¶…è¿‡ 200 ä¸ªå­—ç¬¦ï¼‰
        """
        return await self.ask_ai(user_content, system_content=system_content)

    async def ai_generate_text(self, user_content: str) -> Result[str, str]:
        # ä¸´æ—¶çš„
        system_content = """
        ä½ æ˜¯ä¸€ä½æ–‡æœ¬æ€»ç»“ä¸“å®¶ï¼Œä½ éœ€è¦å°†ç”¨æˆ·å‘é€çš„å†…å®¹æ€»ç»“æˆä¸€ä¸ªç®€çŸ­çš„æ ‡é¢˜ï¼ˆä¸è¦è¶…è¿‡ 200 ä¸ªå­—ç¬¦ï¼‰
        """
        return await self.ask_ai(user_content, system_content=system_content)


async def audio_to_text_by_qwen3_asr(audio_file_path: str | Path):
    """éŸ³é¢‘è½¬æ–‡å­—"""
    logger.debug("æ­£åœ¨å¤„ç†éŸ³é¢‘æ–‡ä»¶ï¼š{}", audio_file_path)
    url = "https://qwen-qwen3-asr-demo.ms.show/"
    client = Client(url)
    job = client.submit(
        audio_file=handle_file(audio_file_path),
        context="",
        language="auto",
        enable_itn=False,
        api_name="/asr_inference"
    )
    loop = asyncio.get_running_loop()
    result = await loop.run_in_executor(get_thread_pool_executor(), lambda :job.result())
    logger.debug("api è¿”å›ç»“æœï¼š{}", result)
    return result[0] if result else ""
